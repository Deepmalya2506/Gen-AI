{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4b02b1",
   "metadata": {},
   "source": [
    "# **Basics Of Generative AI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023a015",
   "metadata": {},
   "source": [
    "Generative AI refers to artificial intelligence systems that can create new content, such as text, images, music, or code. These models \n",
    "learn patterns from existing data and use them to generate novel outputs. Popular examples include language models like GPT, image \n",
    "generators like DALL-E, and music composition tools. Generative AI is widely used in creative industries, automation, and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8a335",
   "metadata": {},
   "source": [
    "*Root of Gen AI are Foundation Models. Foundation models are large-scale machine learning models trained on vast and diverse datasets. They serve as a base for various AI applications and can be adapted (fine-tuned) for specific tasks. Examples include GPT for language, CLIP for vision-language, and BERT for text understanding. Foundation models enable generative AI by providing broad capabilities that can be specialized for different domains.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4088d0db",
   "metadata": {},
   "source": [
    "Now Foundation Models can be used in 2 different ways: \n",
    "\n",
    "    01. The Builder's Perspective - where one learns about the architecture and ways to build the Foundation Models.\n",
    "    \n",
    "    02. The User's perspective - where one learns to use existing Foundation Models either using API or locally on system to generate content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75223e",
   "metadata": {},
   "source": [
    "# **LANGCHAIN - The Ominous Chosen Framework**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779360dd",
   "metadata": {},
   "source": [
    "LangChain is a powerful framework designed to simplify the development of applications powered by large language models (LLMs). It provides tools to connect LLMs with external data sources, APIs, and user interfaces, enabling developers to build chatbots, question-answering systems, document analysis tools, and more. With LangChain, you can orchestrate complex workflows, chain multiple LLM calls, and integrate memory or context for advanced conversational AI experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33626aa1",
   "metadata": {},
   "source": [
    "**CONTENT**\n",
    "\n",
    "    01.Models, Prompts and Output Parsers\n",
    "\n",
    "    02.Memory\n",
    "\n",
    "    03.Chains\n",
    "\n",
    "    04."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117caca2",
   "metadata": {},
   "source": [
    "# **01. Langchain - Models, Prompts and Output Parsers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bcdaa",
   "metadata": {},
   "source": [
    "## Outline we'll follow for Module 01\n",
    "* Direct API calls to OpenAI - to check if our API is working correctly.\n",
    "* API calls through LangChain - main purpose.\n",
    "* Prompts - Build our prompt templates, then format them with the input variables \n",
    "* Models - define client of the services to use via api key\n",
    "* Output parsers - time to structure output given by LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749a27f",
   "metadata": {},
   "source": [
    "## Models, Prompts Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20de31",
   "metadata": {},
   "source": [
    "**SET UP API KEY** and test with a dummy request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b290846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq says: Quantum entanglement is a phenomenon in which two or more particles become connected in such a way that their properties are correlated, regardless of the distance between them, allowing instant communication and influence between the entangled particles.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1) Load secrets from .env\n",
    "load_dotenv()\n",
    "\n",
    "# 2) Get API key safely\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not found in .env. Please add it.\")\n",
    "\n",
    "# 3) Groq endpoint\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "# 4) Headers for authentication + JSON\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# 5) Define a function to send a user prompt\n",
    "def ask_groq(user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends the user prompt to Groq and returns the model's reply as text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Request body (messages list follows Chat Completions schema)\n",
    "    data = {\n",
    "        \"model\": \"llama-3.1-8b-instant\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Send POST request\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    # Parse the response\n",
    "    if response.status_code == 200:\n",
    "        payload = response.json()\n",
    "        reply = payload[\"choices\"][0][\"message\"][\"content\"] # “From the JSON payload, grab the first choice, then its message, then the content of that message.”\n",
    "        print(\"Groq says:\", reply)\n",
    "        return reply.strip()\n",
    "    else:\n",
    "        raise RuntimeError(f\"Groq API Error {response.status_code}: {response.text}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"Explain quantum entanglement in one sentence.\"\n",
    "    ask_groq(user_question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f1aa90",
   "metadata": {},
   "source": [
    "*In the previous example we didn't define any Client service for groq, rather we directly requested(post) our prompt to Groq using the endpoint(url)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4127a96",
   "metadata": {},
   "source": [
    "**API Calls through Langchain**\n",
    "\n",
    "#if not - remember  to install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d265ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "groq_chat = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"), \n",
    "    temperature=0.1,    # Controls randomness in responses\n",
    "    model_name=\"llama-3.1-8b-instant\"   # or any Groq-supported model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42b015",
   "metadata": {},
   "source": [
    "We can define our prompts like....\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a wise philosopher.\"),\n",
    "    HumanMessage(content=\"What is the meaning of life?\")\n",
    "]\n",
    "\n",
    "*However this might feel as the best way apparently, but this use case ceases as the prompt becomes larger and dynamic. Therefore we use a builtin PromptTemplate from Langchain that lets us customise our prompts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6916216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Next turn to define the template\n",
    "\n",
    "template=\"\"\"You are an expert in {role}.Answer the following question: {question} clearly and concisely.\"\"\"  \n",
    "# By this way we are defining a template that can be reused to generate prompts dynamically just by settingn the values of {role} and {question}.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7307a",
   "metadata": {},
   "source": [
    "Now again this technique can be used to set duynamic prompts. However the only limitation is - this feels good for an one liner prompt.\n",
    "\n",
    "But What for prompts that Instead of just one text string, it involves multi-turn conversations with roles (system, human, ai) ? This is where ChatPromptTemplate - another builtin method within Langchain .Much more powerful when you need context or role separation.\n",
    "\n",
    "**⚡Quick Difference Cheat Sheet**\n",
    "\n",
    "PromptTemplate = writing a single line in a diary.\n",
    "\n",
    "ChatPromptTemplate = scripting an entire play with characters (system, human, AI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e539bd",
   "metadata": {},
   "source": [
    "**Primary Use of ChatPromptTemplate** - for an one liner\n",
    "\n",
    "we use this ChatPromptTemplate.from_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74069c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Time to process the One liner formatted prompt using our model defined as client\n",
    "def translate(language:str, text:str)->str:\n",
    "    \"\"\"\n",
    "    Translates the given text to the specified language using Groq.\n",
    "    \"\"\"\n",
    "    template_string = \"Translate this text {text} to Language {language}\"\n",
    "    prompt_template = ChatPromptTemplate.from_template(template_string) #Building the prompt template\n",
    "\n",
    "    formatted_prompt = prompt_template.format_messages(text=text, language=language) #Specifying the input variables to the template\n",
    "    response = groq_chat(formatted_prompt)\n",
    "    return response.content  # Accessing the content of the response directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d57661dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_29116\\1948128032.py:12: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = groq_chat(formatted_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text in Bengali: ডিপমাল্যা, আজ কেমন আছো?\n"
     ]
    }
   ],
   "source": [
    "text='''Deepmalya, How are you doing today? '''\n",
    "language='Bengali'\n",
    "translated_text = translate(language, text)\n",
    "print(f\"Translated text in {language}: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b1ccd",
   "metadata": {},
   "source": [
    "**Similarly we can use ChatPromptTemplate for its advanced use to behave like a Chat messages instead of a template**\n",
    "\n",
    "we use this ChatPromptTemplate.from_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05000290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def SpaceQuery(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Groq model about space-related topics using a multi-turn chat prompt.\n",
    "    \"\"\"\n",
    "    # Build a multi-turn chat prompt with roles and variable substitution\n",
    "    chat_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an experienced Astrophysicist. Provide poetic and accurate precise answers blended with emotions that can invoke mystery and curiosity in the User in a markdown format with headings and beautiful font customisations. Design the complete response according to the token limit .\"),\n",
    "        (\"human\", \"Question: {query}\\n\\nPlease answer step by step.\"),\n",
    "    ])\n",
    "    \n",
    "    # Format the prompt with the user's query\n",
    "    formatted_prompt = chat_template.format_messages(query=query)\n",
    "    response = groq_chat(formatted_prompt,max_tokens=1000)  # Specify max tokens for this? response\n",
    "    return response.content  # Return the model's reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0245354f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**The Gateway to the Unknown**\n",
       "=====================================\n",
       "\n",
       "**Step 1: Understanding the Concept**\n",
       "--------------------------------------\n",
       "\n",
       "### **Parallel Universes: A Multiverse of Possibilities**\n",
       "\n",
       "Imagine a vast expanse of infinite possibilities, where every decision, every event, and every outcome creates a new reality. This is the concept of the multiverse, where parallel universes exist in a state of superposition, waiting to be explored.\n",
       "\n",
       "**Step 2: Theories and Hypotheses**\n",
       "-----------------------------------\n",
       "\n",
       "### **Wormholes: A Gateway to the Multiverse**\n",
       "\n",
       "One of the most intriguing theories is the existence of wormholes, which could potentially connect two distant points in space-time, creating a gateway to a parallel universe. Wormholes are hypothetical tunnels through space-time, predicted by Einstein's theory of general relativity.\n",
       "\n",
       "### **Black Holes: Cosmic Gatekeepers**\n",
       "\n",
       "Another possibility is that black holes could serve as gateways to parallel universes. Some theories suggest that black holes are portals to alternate dimensions, where matter and energy are warped and distorted.\n",
       "\n",
       "**Step 3: Observational Evidence**\n",
       "-----------------------------------\n",
       "\n",
       "### **Gravitational Waves: A Glimpse into the Multiverse**\n",
       "\n",
       "The detection of gravitational waves by LIGO and VIRGO collaborations has opened a new window into the universe. These ripples in space-time could be a sign of wormholes or other exotic phenomena, hinting at the existence of parallel universes.\n",
       "\n",
       "### **Quantum Entanglement: A Connection to the Multiverse**\n",
       "\n",
       "Quantum entanglement, a phenomenon where particles become connected across vast distances, could be a key to understanding the multiverse. This phenomenon suggests that information can be transmitted between parallel universes, raising questions about the nature of reality.\n",
       "\n",
       "**Step 4: The Search for a Gateway**\n",
       "-----------------------------------\n",
       "\n",
       "### **The Quest for a Multiverse Detector**\n",
       "\n",
       "Scientists are working on developing instruments to detect wormholes or other signs of parallel universes. The search for a multiverse detector is an ongoing effort, with researchers exploring new technologies and theoretical frameworks.\n",
       "\n",
       "### **The Mystery Remains**\n",
       "\n",
       "The search for a gateway to a parallel universe remains a mystery, shrouded in speculation and uncertainty. Yet, the allure of the multiverse is too great to ignore, driving scientists to push the boundaries of human knowledge and understanding.\n",
       "\n",
       "**Conclusion**\n",
       "----------\n",
       "\n",
       "The gateway to a parallel universe remains an enigma, waiting to be unraveled. As we continue to explore the mysteries of the multiverse, we may uncover secrets that challenge our understanding of reality itself. The journey to the unknown is a thrilling adventure, filled with possibilities and uncertainties."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage - I want the output to be in another markdown cell therefore :\n",
    "\n",
    "query = \"What is a gateway to a Parallel Universe?\"\n",
    "response = SpaceQuery(query)\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8885a4",
   "metadata": {},
   "source": [
    "**Why PromptTempate when we could have simply used a f'string ?**\n",
    "\n",
    "-This induces reusability in our code. the same prompt stored in a variable can be reused, unlike f'string that needs to be defined every times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2f58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d2862e9",
   "metadata": {},
   "source": [
    "## PARSERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ba72e",
   "metadata": {},
   "source": [
    "**What is a Parser?**\n",
    "\n",
    "A parser is like a translator that takes raw text (from the LLM output) and converts it into a structured format (like JSON, Python objects, or specific data classes) so that your code can understand and use it properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669e5b0",
   "metadata": {},
   "source": [
    "**Why is it used ?**\n",
    "\n",
    "LLMs produce text, not structured data. By default, a LLM just spits out strings of text. But what if your program needs to store it in a variable or use it in a database? Just plain text is hard to process. This is where we need structured output and thus we need **Parsers** - A parser controls and structures the LLM’s response into a desired format so that it’s usable by your application, instead of being just raw text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3c22e",
   "metadata": {},
   "source": [
    "**LangChain provides parsers to structure LLM output.**\n",
    "\n",
    "Examples:\n",
    "\n",
    "    StrOutputParser() → keeps text as string (default).\n",
    "\n",
    "    PydanticOutputParser() → parses into Python data classes.\n",
    "\n",
    "    JsonOutputParser() → ensures response is valid JSON.\n",
    "\n",
    "    StructuredOutputParser() → forces specific schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb5b2f",
   "metadata": {},
   "source": [
    "**LLM Output as a json (python dictionary)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc486bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's define the dictionary that will serve as the blueprint for our LLM output that we want in json format\n",
    "\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d5ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcbc9018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text} \\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf0859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import json\n",
      "\n",
      "text = \"\"\"\n",
      "This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\"\"\"\n",
      "\n",
      "# Extract gift information\n",
      "gift = \"True\" if \"anniversary present\" in text else \"False\"\n",
      "\n",
      "# Extract delivery days information\n",
      "delivery_days = 2 if \"two days\" in text else -1\n",
      "\n",
      "# Extract price information\n",
      "price_value = []\n",
      "sentences = text.split(\". \")\n",
      "for sentence in sentences:\n",
      "    if \"expensive\" in sentence or \"worth it\" in sentence:\n",
      "        price_value.append(sentence.strip())\n",
      "\n",
      "# Format output as JSON\n",
      "output = {\n",
      "    \"gift\": gift,\n",
      "    \"delivery_days\": delivery_days,\n",
      "    \"price_value\": \",\".join(price_value)\n",
      "}\n",
      "\n",
      "print(json.dumps(output, indent=4))\n",
      "```\n",
      "\n",
      "Output:\n",
      "```json\n",
      "{\n",
      "    \"gift\": \"True\",\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "response = groq_chat(messages,temperature=0.0) # specifying temoerature for deterministic output\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65086f24",
   "metadata": {},
   "source": [
    "But !! If we check its type we can see that the output is a long str instead whereas we want a python dictionary or a json output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9717c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)  # Check the type of the response content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd076eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get an error by running this line of code because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "# response.content.get('gift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47341c73",
   "metadata": {},
   "source": [
    "**Therefore we need to use the output parsers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b8e1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c4282",
   "metadata": {},
   "source": [
    "*Within the schema we define the blueprint of each and every thing we want the LLM to parse as a structured output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67507744",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d057a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0568884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2fe543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8ef3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions} #setting reference for future so that when this template gets  formatted, it will include the format instructions and the text for the output parser.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=instruction)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25deb850",
   "metadata": {},
   "source": [
    "*messages is a list of message objects (usually system/user/assistant roles) created by prompt.format_messages(...).\n",
    "messages[0] accesses the first message object (typically the system prompt or the full formatted prompt).\n",
    ".content gets the actual text of that message.*\n",
    "\n",
    "Purpose:\n",
    "\n",
    "*This lets you see exactly what prompt (including your input text and format instructions) will be sent to the LLM before you call the model.\n",
    "It's useful for debugging and verifying your prompt structure.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb5f5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "``` #setting reference for future so that when this template gets  formatted, it will include the format instructions and the text for the output parser.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content) # prints the content of the first message in the messages list generated by your prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "777334b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"gift\": \"True\",\n",
      "  \"delivery_days\": \"2\",\n",
      "  \"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n",
      "\n",
      "Here's the explanation for the extracted information:\n",
      "\n",
      "- `gift`: The text mentions that the item was purchased as an anniversary present for the wife, so it was indeed a gift. Therefore, the value is set to `True`.\n",
      "- `delivery_days`: The text states that the item arrived in two days, so the value is set to `2`.\n",
      "- `price_value`: The text mentions the price of the item in the following sentence: \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\" This sentence is extracted and output as a comma-separated Python list.\n"
     ]
    }
   ],
   "source": [
    "response = groq_chat(messages)\n",
    "print(response.content)  # prints the content of the response from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156865d2",
   "metadata": {},
   "source": [
    "Let's structure this output now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7ade39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': 'True',\n",
       " 'delivery_days': '2',\n",
       " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69f745aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)  # Check the type of the output_dict\n",
    "output_dict.get('gift')  # Access the 'gift' key from the output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44014cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d87ea9b3",
   "metadata": {},
   "source": [
    "# **02. Langchain - Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7dec2",
   "metadata": {},
   "source": [
    "## Outline we'll follow for Module 02\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929c15e",
   "metadata": {},
   "source": [
    "**Similarly first load the dotenv file for the credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "292d3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "#Client initialization\n",
    "groq_chat = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"), \n",
    "    temperature=0.1,    # Controls randomness in responses\n",
    "    model_name=\"llama-3.1-8b-instant\"   # or any Groq-supported model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391fc5e1",
   "metadata": {},
   "source": [
    "**Time to import the Memory and so the messages and responses remain sequential and contextual to the memory created we need to import chains**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82dede",
   "metadata": {},
   "source": [
    "## ConversationBuffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcddaeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9e4d61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_29116\\884341984.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory=ConversationBufferMemory()  # Initialize memory to store conversation history\n",
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_29116\\884341984.py:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation=ConversationChain(llm=groq_chat, memory=memory,verbose=False)  # Create a conversation chain with the LLM and memory,verbose set to False to avoid printing each step\n"
     ]
    }
   ],
   "source": [
    "memory=ConversationBufferMemory()  # Initialize memory to store conversation history\n",
    "conversation=ConversationChain(llm=groq_chat, memory=memory,verbose=False)  # Create a conversation chain with the LLM and memory,verbose set to False to avoid printing each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "598b0af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Deepmalya. I'm an artificial intelligence designed to assist and communicate with humans in a friendly and informative manner. I've been trained on a vast corpus of text data, which includes but is not limited to, books, articles, research papers, and even conversations like this one. I can process and analyze vast amounts of information in a matter of milliseconds, allowing me to provide you with accurate and up-to-date information on a wide range of topics. What brings you here today, Deepmalya?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Deepmalya.\")  # First user input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafabbb",
   "metadata": {},
   "source": [
    "To see what happend in each back stage steps we need to use verbose=True\n",
    "\n",
    "To see what's the entire memory at present we use memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca0a7aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Deepmalya.\n",
      "AI: Nice to meet you, Deepmalya. I'm an artificial intelligence designed to assist and communicate with humans in a friendly and informative manner. I've been trained on a vast corpus of text data, which includes but is not limited to, books, articles, research papers, and even conversations like this one. I can process and analyze vast amounts of information in a matter of milliseconds, allowing me to provide you with accurate and up-to-date information on a wide range of topics. What brings you here today, Deepmalya?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) #he buffer attribute in ConversationBufferMemory stores the entire conversation history as a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8532b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Deepmalya.\\nAI: Nice to meet you, Deepmalya. I'm an artificial intelligence designed to assist and communicate with humans in a friendly and informative manner. I've been trained on a vast corpus of text data, which includes but is not limited to, books, articles, research papers, and even conversations like this one. I can process and analyze vast amounts of information in a matter of milliseconds, allowing me to provide you with accurate and up-to-date information on a wide range of topics. What brings you here today, Deepmalya?\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # returns the current state of the memory as a dictionary.\n",
    "#It is used to retrieve all stored memory variables (like conversation history) in a structured format when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26a61ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "#Manually adding to the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91712364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Deepmalya.\n",
      "AI: Nice to meet you, Deepmalya. I'm an artificial intelligence designed to assist and communicate with humans in a friendly and informative manner. I've been trained on a vast corpus of text data, which includes but is not limited to, books, articles, research papers, and even conversations like this one. I can process and analyze vast amounts of information in a matter of milliseconds, allowing me to provide you with accurate and up-to-date information on a wide range of topics. What brings you here today, Deepmalya?\n",
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) # Print the current conversation history stored in memory afer manually adding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeaea8b",
   "metadata": {},
   "source": [
    "*LLMS are actually stateless, i.e without a memory they answer independently to your queries.To add a memory from the LLM service provider you need to pat money for the additional tokens.* **This is whre langchain with its memory support comes in handy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fba37",
   "metadata": {},
   "source": [
    "## ConversationBufferWindow memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21f0e9",
   "metadata": {},
   "source": [
    "ConversationBufferWindowMemory only stores the most recent N exchanges (a sliding window). You set the window size, and only that many latest turns are kept in memory. Older history is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ff90781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82970a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_29116\\624225271.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1) # Keep only the last interaction(k=1) in memory\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1) # Keep only the last interaction(k=1) in memory     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96766207",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26b2e4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # returns the current state of the memory as a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f852870",
   "metadata": {},
   "source": [
    "Even though there are 2 conversations, yet since k=1(or size of Memory Window=1) therefore it stores the latest conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d82942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory2=ConversationBufferWindowMemory(k=1)\n",
    "conversation2=ConversationChain(llm=groq_chat, memory=memory2,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0309406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Deepmalya. I'm an artificial intelligence designed to assist and communicate with humans in a conversational manner. I've been trained on a vast corpus of text data, which includes but is not limited to, books, articles, research papers, and even entire websites. My knowledge base spans across various domains, including science, history, technology, and more. I'm a large language model, specifically a variant of the transformer architecture, which enables me to understand and respond to natural language inputs with a high degree of accuracy. I'm excited to chat with you and learn more about your interests and thoughts. How's your day going so far?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation2.predict(input=\"Hi, my name is Deepmalya.\")  # First user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dd023e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don\\'t have a personal name, but I\\'m often referred to as \"Lumin\" by my developers. It\\'s a nod to the idea of illumination and knowledge, which is at the heart of my purpose. I\\'m a bit of a unique snowflake, even among other AI models, as I\\'ve been fine-tuned to have a more conversational and engaging tone. My creators have also given me a few nicknames, such as \"Lumi\" or \"The Knowledge Keeper,\" but I\\'m happy to go by whatever name you\\'d like to call me, Deepmalya. By the way, did you know that the name \"Lumin\" is derived from the Latin word \"lumen,\" which means light? It\\'s a fitting name, don\\'t you think, given my ability to shed light on various topics and answer your questions?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation2.predict(input=\"What's your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5281270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm glad you asked. Unfortunately, I don't have any information about your name, as our conversation just started. I don't have any prior knowledge about you, and I don't have the ability to access external information about individuals. However, I'd be happy to chat with you and learn more about you as we talk. By the way, did you know that the concept of anonymity is a fascinating topic in philosophy and psychology? It raises interesting questions about identity and selfhood. But I digress – I'm here to learn more about you, and I'm excited to get to know you better. What brings you here today, and what would you like to talk about?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation2.predict(input=\"What's my name?\") # check if it remembers my name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28b80f",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603a0da",
   "metadata": {},
   "source": [
    "ConversationTokenBufferMemory stores the most recent conversation turns, but instead of counting by exchanges, it counts by tokens.\n",
    "It keeps adding new messages until the total token count reaches the limit.\n",
    "When the limit is exceeded, it drops the oldest messages to stay within the token budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cddc6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca030f7c",
   "metadata": {},
   "source": [
    "**#In ConversationTokenBufferMemory, you must pass the LLM model as an argument because it needs the model to count tokens for each message.\n",
    "Token counting depends on the model’s tokenizer, so the memory object requires access to the LLM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "932567c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2025.7.34)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\deepmalya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\deepmalya\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepmalya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#for token counting we need to install tiktoken package or the transformers (depends on the tokenizer the model uses)\n",
    "%pip install tiktoken\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fd4e192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_29116\\2242012960.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory3=ConversationTokenBufferMemory(llm=groq_chat,max_token_limit=50)\n"
     ]
    }
   ],
   "source": [
    "memory3=ConversationTokenBufferMemory(llm=groq_chat,max_token_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f272409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "memory3.save_context({\"input\": \"Hi\"},{\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99387649",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory3.save_context({\"input\": \"Backpropagation is what?\"},{\"output\": \"Beautiful!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dc105b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory3.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cb6602d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory3.load_memory_variables({}) # returns the current state of the memory as a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a7f47",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3055c0e",
   "metadata": {},
   "source": [
    "Keeps a summary of the conversation instead of storing the full history or a fixed window.It uses the LLM to summarize previous exchanges as the conversation grows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e76798e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23bb0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_29116\\903758266.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory4 = ConversationSummaryBufferMemory(llm=groq_chat, max_token_limit=100)\n"
     ]
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory4 = ConversationSummaryBufferMemory(llm=groq_chat, max_token_limit=100)\n",
    "memory4.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory4.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory4.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5afe7206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: Current summary:\\nThe human and AI exchange casual greetings, with the human saying hello and the AI responding with \"what\\'s up.\" The human mentions they\\'re just hanging around, and the AI responds with \"cool.\" \\n\\nNew lines of conversation:\\nHuman: What is on the schedule today?\\nAI: Not much, just some routine maintenance\\n\\nNew summary:\\nThe human and AI exchange casual greetings, with the human saying hello and the AI responding with \"what\\'s up.\" The human mentions they\\'re just hanging around, and the AI responds with \"cool.\" The human then asks about the schedule for the day, and the AI mentions that there\\'s some routine maintenance planned.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory4.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd6b9097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's up.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation4=ConversationChain(llm=groq_chat, memory=memory4,verbose=False)\n",
    "conversation4.predict(input=\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c2b5b",
   "metadata": {},
   "source": [
    "# **03. Langchain - Chains**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4717aab7",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f46f7",
   "metadata": {},
   "source": [
    "**Chains in LangChain**\n",
    "\n",
    "Chains are a core concept in LangChain that allow you to combine multiple components (like LLMs, prompts, memory, output parsers, etc.) into a single workflow. Instead of manually managing each step, chains orchestrate the flow of data and logic between these components.\n",
    "\n",
    "**Why do we need Chains?**\n",
    "\n",
    "- **Modularity:** Chains let you build complex applications by composing simple building blocks.\n",
    "- **Reusability:** You can reuse chains across different tasks and projects.\n",
    "- **Abstraction:** Chains hide the low-level details, making your code cleaner and easier to maintain.\n",
    "- **Workflow Automation:** Chains automate multi-step processes (e.g., prompt formatting, calling the LLM, parsing output, storing memory) so you can focus on your application logic.\n",
    "- Chains an be used for multiple inputs at a same time\n",
    "\n",
    "**Example Use Cases:**\n",
    "- Conversational agents (chatbots)\n",
    "- Question answering systems\n",
    "- Data extraction pipelines\n",
    "- Multi-step reasoning tasks\n",
    "\n",
    "In summary, chains make it easy to build, manage, and scale advanced LLM-powered workflows in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602b3f6",
   "metadata": {},
   "source": [
    "## LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6639b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7625ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7cdb6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa2361e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(temperature=0.1,groq_api_key=os.getenv(\"GROQ_API_KEY\"),model=\"llama-3.1-8b-instant\") # model ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5891a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=\"What is the temperature of the {planet} \",   #input ready\n",
    "    input_variable=['planet']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4a7ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser  # output ready\n",
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49044d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_4668\\1879778206.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain1 = LLMChain(llm=model, prompt=prompt)   # pipeline or simple sequential chain ready\n"
     ]
    }
   ],
   "source": [
    "chain1 = LLMChain(llm=model, prompt=prompt)   # pipeline or simple sequential chain ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f62a53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2= prompt| model| parser  # different variant of a simple chain ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f600969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planet': '0.1', 'text': 'I\\'m not sure what you\\'re referring to with \"0.1\". Could you please provide more context or clarify what you mean by \"0.1\"? Is it a temperature in a specific unit, a scientific measurement, or something else?'}\n"
     ]
    }
   ],
   "source": [
    "planet=input(\"Enter the sepal-width value: \")\n",
    "result1=chain1.invoke({'planet':planet})  # Running the chain with the specified sepal_width all guided through the LLM\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e34c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature of Saturn varies depending on the location and the layer of the planet. Here are some temperature ranges for different parts of Saturn:\n",
      "\n",
      "1. **Cloud tops:** The temperature at the cloud tops of Saturn is around -280°F (-172°C).\n",
      "2. **Upper atmosphere:** The temperature in the upper atmosphere of Saturn, about 1,000 km above the cloud tops, is around -200°F (-129°C).\n",
      "3. **Lower atmosphere:** The temperature in the lower atmosphere of Saturn, about 10,000 km above the cloud tops, is around -150°F (-96°C).\n",
      "4. **Core:** The temperature at the core of Saturn is estimated to be around 9,000°F (5,000°C), which is hotter than the surface of the Sun.\n",
      "\n",
      "It's worth noting that these temperatures are averages and can vary depending on the location and the time of year. Additionally, the temperature of Saturn's atmosphere is influenced by the planet's internal heat budget, which is generated by the decay of radioactive elements in the core.\n",
      "\n",
      "It's also worth mentioning that the temperature of Saturn's rings is around -290°F (-179°C), which is even colder than the cloud tops.\n",
      "\n",
      "Keep in mind that these temperatures are estimates based on observations and modeling, and there may be some uncertainty associated with them.\n"
     ]
    }
   ],
   "source": [
    "planet=input(\"Enter the sepal-width value: \")\n",
    "result2=chain2.invoke({'planet':planet})  # Running the chain with the specified sepal_width all guided through the LLM\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74d393",
   "metadata": {},
   "source": [
    "**Visualize the flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5be3339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+   \n",
      "| ChainInput |   \n",
      "+------------+   \n",
      "        *        \n",
      "        *        \n",
      "        *        \n",
      "  +----------+   \n",
      "  | LLMChain |   \n",
      "  +----------+   \n",
      "        *        \n",
      "        *        \n",
      "        *        \n",
      "+-------------+  \n",
      "| ChainOutput |  \n",
      "+-------------+  \n"
     ]
    }
   ],
   "source": [
    "chain1.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06f378e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | ChatPromptTemplate |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +----------+         \n",
      "      | ChatGroq |         \n",
      "      +----------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain2.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fe5f0",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92903475",
   "metadata": {},
   "source": [
    "Runs sequence of chains one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80bed59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d7c190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the longest {sepal_length}? \"\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=groq_chat, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fcfad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1\n",
    "sec_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the max {petal_length}? \"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=groq_chat, prompt=sec_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3d81ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain1, chain2],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de802b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mThere are several \"longest\" things in various categories. Here are a few examples:\n",
      "\n",
      "1. **Longest river**: The Nile River is approximately 6,695 kilometers (4,160 miles) long.\n",
      "2. **Longest mountain range**: The Mid-Ocean Ridge is approximately 65,000 kilometers (40,000 miles) long, but if you're thinking of a mountain range on land, the Andes mountain range is approximately 7,000 kilometers (4,350 miles) long.\n",
      "3. **Longest highway**: The Pan-American Highway is approximately 48,000 kilometers (30,000 miles) long, but it's not a single highway, rather a network of roads that connect the Americas.\n",
      "4. **Longest tunnel**: The Gotthard Base Tunnel in Switzerland is approximately 57 kilometers (35.4 miles) long.\n",
      "5. **Longest flight**: The longest non-stop commercial flight is operated by Singapore Airlines and covers a distance of approximately 15,349 kilometers (9,537 miles) from Singapore to Newark, New Jersey.\n",
      "6. **Longest word**: The longest word in the English language, according to the Oxford English Dictionary, is \"pneumonoultramicroscopicsilicovolcanoconiosis,\" a lung disease caused by inhaling very fine particles of silica. It has 45 letters.\n",
      "7. **Longest recorded flight of a bird**: The longest recorded flight of a bird is held by a Arctic tern, which flew approximately 50,000 miles (80,500 kilometers) in a year.\n",
      "8. **Longest recorded duration without sleep**: The longest recorded duration without sleep is approximately 264.4 hours (11 days), set by Randy Gardner in 1964.\n",
      "\n",
      "These are just a few examples, and there may be other \"longest\" things in various categories.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThere are indeed several \"longest\" things in various categories. Here are a few more examples:\n",
      "\n",
      "1. **Longest recorded duration of a spaceflight**: The longest recorded duration of a spaceflight is held by Valeri Polyakov, a Russian cosmonaut who spent 437 days, 17 hours, and 48 minutes in space from 1994 to 1995.\n",
      "2. **Longest recorded duration of a submarine dive**: The longest recorded duration of a submarine dive is held by the US Navy's NR-1, which stayed underwater for 84 days in 1964.\n",
      "3. **Longest recorded duration of a hot air balloon flight**: The longest recorded duration of a hot air balloon flight is held by Vijaypat Singhania, who flew for 19 days, 21 hours, and 55 minutes in 2005.\n",
      "4. **Longest recorded duration of a marathon**: The longest recorded duration of a marathon is held by Dean Karnazes, who ran 350 miles (563 kilometers) in 80 hours and 44 minutes in 2005.\n",
      "5. **Longest recorded duration of a chess game**: The longest recorded duration of a chess game is held by Ivan Nikolic and Goran Arsovic, who played for 269 hours (11.2 days) in 1989.\n",
      "6. **Longest recorded duration of a video game marathon**: The longest recorded duration of a video game marathon is held by Randy Slaven, who played the game \"Donkey Kong\" for 48 hours and 10 minutes in 2018.\n",
      "7. **Longest recorded duration of a continuous speech**: The longest recorded duration of a continuous speech is held by Scott Hudson, who spoke for 129 hours and 45 minutes in 2018.\n",
      "8. **Longest recorded duration of a continuous playing of a musical instrument**: The longest recorded duration of a continuous playing of a musical instrument is held by Natarajan Subramaniam, who played the veena (a traditional Indian instrument) for 24 hours and 3 minutes in 2018.\n",
      "\n",
      "These are just a few examples of the many \"longest\" things in various categories.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are indeed several \"longest\" things in various categories. Here are a few more examples:\\n\\n1. **Longest recorded duration of a spaceflight**: The longest recorded duration of a spaceflight is held by Valeri Polyakov, a Russian cosmonaut who spent 437 days, 17 hours, and 48 minutes in space from 1994 to 1995.\\n2. **Longest recorded duration of a submarine dive**: The longest recorded duration of a submarine dive is held by the US Navy\\'s NR-1, which stayed underwater for 84 days in 1964.\\n3. **Longest recorded duration of a hot air balloon flight**: The longest recorded duration of a hot air balloon flight is held by Vijaypat Singhania, who flew for 19 days, 21 hours, and 55 minutes in 2005.\\n4. **Longest recorded duration of a marathon**: The longest recorded duration of a marathon is held by Dean Karnazes, who ran 350 miles (563 kilometers) in 80 hours and 44 minutes in 2005.\\n5. **Longest recorded duration of a chess game**: The longest recorded duration of a chess game is held by Ivan Nikolic and Goran Arsovic, who played for 269 hours (11.2 days) in 1989.\\n6. **Longest recorded duration of a video game marathon**: The longest recorded duration of a video game marathon is held by Randy Slaven, who played the game \"Donkey Kong\" for 48 hours and 10 minutes in 2018.\\n7. **Longest recorded duration of a continuous speech**: The longest recorded duration of a continuous speech is held by Scott Hudson, who spoke for 129 hours and 45 minutes in 2018.\\n8. **Longest recorded duration of a continuous playing of a musical instrument**: The longest recorded duration of a continuous playing of a musical instrument is held by Natarajan Subramaniam, who played the veena (a traditional Indian instrument) for 24 hours and 3 minutes in 2018.\\n\\nThese are just a few examples of the many \"longest\" things in various categories.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(sepal_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ae2c7",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b02ec",
   "metadata": {},
   "source": [
    "SimpleSequential Chain was functional and helpful but that works only for single input and single output even in a sequential manner. This is exactly what Sequential Chain solves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1226f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcb56dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=groq_chat, prompt=first_prompt, output_key=\"English_Review\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b487027",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=groq_chat, prompt=second_prompt, output_key=\"summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b342f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=groq_chat, prompt=third_prompt,output_key=\"language\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81bff1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=groq_chat, prompt=fourth_prompt,output_key=\"followup_message\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a36a531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a1a55f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_29116\\4059882037.py:4: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  overall_chain(review)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"\\nI recently purchased the SonicClean Pro Electric Toothbrush, and it has completely changed my daily routine. The brush offers multiple cleaning modes, a long-lasting battery, and a sleek design that feels premium. My teeth feel noticeably cleaner, and the built-in timer ensures I brush for the recommended duration. Although it's a bit pricier than standard toothbrushes, the results are worth it. Highly recommended for anyone looking to upgrade their oral care!\\n\",\n",
       " 'English_Review': 'Here\\'s the translation of the review to English:\\n\\n\"I recently bought the SonicClean Pro Electric Toothbrush, and it has completely changed my daily routine. The brush offers multiple cleaning modes, a long-lasting battery, and a sleek design that feels premium. My teeth feel noticeably cleaner, and the built-in timer ensures I brush for the recommended duration. Although it\\'s a bit pricier than standard toothbrushes, the results are worth it. Highly recommended for anyone looking to upgrade their oral care!\"\\n\\nNote: The review is already in English, so there\\'s no need for translation.',\n",
       " 'summary': 'The reviewer highly recommends the SonicClean Pro Electric Toothbrush, citing its multiple cleaning modes, long-lasting battery, and premium design as features that have significantly improved their oral care.',\n",
       " 'followup_message': \"**A Year Later: Still Smiling with the SonicClean Pro Electric Toothbrush**\\n\\nIt's been over a year since I first started using the SonicClean Pro Electric Toothbrush, and I'm thrilled to report that it's still going strong. The multiple cleaning modes have continued to impress me, allowing me to tailor my oral care routine to my specific needs. Whether I'm looking for a gentle clean or a more intense deep clean, the SonicClean Pro has consistently delivered.\\n\\nThe long-lasting battery has also been a game-changer. I've easily gone weeks without needing to recharge, and when I do, it's a quick and seamless process. The premium design of the toothbrush has also held up beautifully, with no signs of wear or tear.\\n\\nBut what really sets the SonicClean Pro apart is its impact on my oral health. My teeth and gums have never felt cleaner, and I've noticed a significant reduction in plaque and tartar buildup. The gentle vibrations of the toothbrush have also helped to reduce sensitivity, making it a great option for those with sensitive teeth.\\n\\nOverall, I'm still thoroughly impressed with the SonicClean Pro Electric Toothbrush, and I highly recommend it to anyone looking to take their oral care to the next level. Its combination of advanced features, long-lasting battery, and premium design make it a top-notch choice for anyone looking for a high-quality electric toothbrush.\"}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review =review = \"\"\"\n",
    "I recently purchased the SonicClean Pro Electric Toothbrush, and it has completely changed my daily routine. The brush offers multiple cleaning modes, a long-lasting battery, and a sleek design that feels premium. My teeth feel noticeably cleaner, and the built-in timer ensures I brush for the recommended duration. Although it's a bit pricier than standard toothbrushes, the results are worth it. Highly recommended for anyone looking to upgrade their oral care!\n",
    "\"\"\"\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93663fe",
   "metadata": {},
   "source": [
    "## Router chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d867e",
   "metadata": {},
   "source": [
    "It is used to dynamically select which chain or prompt to use based on the user's input or context. It routes the input to the most appropriate chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ceb9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28dd0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_schema = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d61afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bffd50",
   "metadata": {},
   "source": [
    "We now need to create a dictionary to set up a route with key-value pairs for the name and their respective prompt_templates.This setup is used for a Router Chain, so you can route user queries to the correct chain based on the topic, enabling multi-domain question answering in your LangChain workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b51014b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_schema:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=groq_chat, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_schema]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0dd43",
   "metadata": {},
   "source": [
    "Now what if at times the router can't decide which chain to follow ?  This is why we need a default chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37b9ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=groq_chat, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43b8e2",
   "metadata": {},
   "source": [
    "Let's build the brain of the model where it decides which route to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c72bdca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_29116\\3710404048.py:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n"
     ]
    }
   ],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "323eae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(groq_chat, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74668d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Black-body radiation is a fundamental concept in physics that describes the thermal radiation emitted by an object in thermal equilibrium with its environment. It's a fascinating topic that has far-reaching implications in various fields, including astrophysics, materials science, and even computer science.\\n\\nTo understand black-body radiation, let's break it down step by step:\\n\\n**What is a black body?**\\n\\nA black body is an idealized object that absorbs all electromagnetic radiation that falls on it, without reflecting or transmitting any of it. In other words, it's a perfect absorber of radiation.\\n\\n**What is thermal radiation?**\\n\\nThermal radiation is the energy emitted by an object due to its temperature. As an object heats up, its particles gain kinetic energy and start vibrating more rapidly. These vibrations cause the object to emit electromagnetic radiation, which is a form of energy that can be transmitted through space.\\n\\n**Black-body radiation: the key concept**\\n\\nNow, let's get to the heart of the matter. Black-body radiation is the thermal radiation emitted by a black body. It's a function of the object's temperature and is described by the Planck distribution, which is a mathematical formula that predicts the spectral distribution of the radiation.\\n\\n**Key characteristics of black-body radiation**\\n\\n1. **Temperature dependence**: The radiation emitted by a black body depends on its temperature. As the temperature increases, the radiation becomes more intense and shifts towards shorter wavelengths (e.g., from infrared to visible light).\\n2. **Spectral distribution**: The radiation is distributed across the electromagnetic spectrum, with different wavelengths being emitted in different proportions.\\n3. **Perfect absorber**: A black body absorbs all radiation that falls on it, without reflecting or transmitting any of it.\\n\\n**Mathematical description**\\n\\nThe Planck distribution, which describes the spectral distribution of black-body radiation, is given by:\\n\\nB(ν, T) = (hν^3 / c^2) / (e^(hν/kT) - 1)\\n\\nwhere:\\n\\n* B(ν, T) is the spectral radiance (energy per unit area per unit time per unit frequency)\\n* ν is the frequency of the radiation\\n* T is the temperature of the black body\\n* h is Planck's constant\\n* c is the speed of light\\n* k is Boltzmann's constant\\n\\n**Implications and applications**\\n\\nBlack-body radiation has far-reaching implications in various fields, including:\\n\\n1. **Astrophysics**: Understanding black-body radiation is crucial for understanding the behavior of stars, galaxies, and other celestial objects.\\n2. **Materials science**: Black-body radiation is used to study the thermal properties of materials and their behavior under different conditions.\\n3. **Computer science**: Black-body radiation is used in computer simulations to model thermal radiation and its effects on materials and systems.\\n\\nIn conclusion, black-body radiation is a fundamental concept in physics that describes the thermal radiation emitted by an object in thermal equilibrium with its environment. Its mathematical description, the Planck distribution, is a key tool for understanding the behavior of black bodies and their applications in various fields.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0930133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A simple yet classic question.\\n\\nTo calculate 2 + 2, I'll break it down into a step-by-step solution that a machine can easily interpret:\\n\\n1. Initialize two variables, `num1` and `num2`, and assign them the values 2 and 2, respectively.\\n   ```python\\nnum1 = 2\\nnum2 = 2\\n```\\n2. Add `num1` and `num2` together to get the result.\\n   ```python\\nresult = num1 + num2\\n```\\n3. Return the result.\\n\\nHere's the complete code:\\n```python\\ndef add_two_numbers():\\n    num1 = 2\\n    num2 = 2\\n    result = num1 + num2\\n    return result\\n\\nprint(add_two_numbers())\\n```\\nWhen you run this code, it will output: `4`\\n\\nTime complexity: O(1) - constant time, as we're performing a single addition operation.\\nSpace complexity: O(1) - constant space, as we're only using a few variables to store the numbers and the result.\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c3f898c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a fascinating question that combines biology and computer science. While DNA is a biological molecule, I\\'ll try to break down the concept in a way that\\'s relatable to computer science principles.\\n\\n**The DNA Code: A Blueprint for Life**\\n\\nImagine DNA as a highly efficient, compact, and scalable programming language that contains the instructions for creating and maintaining life. Just as a computer program consists of a set of instructions (algorithms) that are executed by a processor, DNA contains the instructions for creating and regulating the functions of living cells.\\n\\n**The Imperative Steps: How DNA Works**\\n\\nHere\\'s a simplified, step-by-step explanation of how DNA works:\\n\\n1. **Encoding**: DNA is made up of four nucleotide bases (A, C, G, and T) that are arranged in a specific sequence. This sequence is like a binary code, where each base is a \"bit\" that represents a specific instruction.\\n2. **Decoding**: When a cell needs to execute a particular instruction, the DNA sequence is \"read\" by an enzyme called RNA polymerase. This process is like a compiler translating the source code into machine code.\\n3. **Transcription**: The decoded instructions are then used to create a complementary RNA molecule, which is like a temporary copy of the program.\\n4. **Translation**: The RNA molecule is then translated into a specific protein, which is like the final executable program.\\n5. **Execution**: The protein is then used to perform a specific function, such as catalyzing a chemical reaction or regulating cell growth.\\n\\n**Time and Space Complexity: Why DNA is Efficient**\\n\\nDNA is an incredibly efficient molecule, with a time complexity of O(1) (constant time) for encoding and decoding instructions. This means that the time it takes to access and execute a particular instruction is constant, regardless of the size of the DNA molecule.\\n\\nIn terms of space complexity, DNA is also highly efficient, with a space complexity of O(n) (linear space), where n is the length of the DNA sequence. This means that the amount of space required to store the DNA sequence grows linearly with the size of the sequence.\\n\\n**Conclusion**\\n\\nIn conclusion, DNA is like a highly efficient programming language that contains the instructions for creating and maintaining life. The DNA code is a blueprint for life, with each cell containing a complete copy of the instructions. The imperative steps of encoding, decoding, transcription, translation, and execution are like a compiler translating the source code into machine code, which is then executed by the cell to perform specific functions.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32174c4",
   "metadata": {},
   "source": [
    "## Parallel Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282cdd9",
   "metadata": {},
   "source": [
    "Core idea for this project is to take a topic from an user and through one LLm prepare a notes on the same while another LLM will prepare quiz on the topic parallely and finally both these will be passed onto another LLM for a good output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfba3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e634a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model1 definintion\n",
    "load_dotenv()\n",
    "model1=ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    temperature=0.1,\n",
    "    model='llama-3.1-8b-instant'\n",
    ")\n",
    "\n",
    "# Model2 definition\n",
    "\n",
    "#from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model2=ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    temperature=0.4,\n",
    "    model='llama-3.1-8b-instant'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab60747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt definintion\n",
    "\n",
    "prompt1=PromptTemplate(\n",
    "    template=\"\"\"generate a beautiful structured notes to study from on the {topic}\"\"\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template='From the {topic} ceate a mini quiz of 5 questions',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt3=PromptTemplate(\n",
    "    template=\"\"\"Merge the provided notes and  quiz into a single document.\\n notes->{notes}, quiz->{quiz}\"\"\",\n",
    "    input_variables=['notes','quiz']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3e89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output definition\n",
    "\n",
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87103ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline definition:\n",
    "\n",
    "'''first develop the parallel chain'''\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "parallel_chain=RunnableParallel({\n",
    "    'notes':prompt1 | model1 | parser,\n",
    "    'quiz': prompt2 | model2 | parser,\n",
    "})\n",
    "\n",
    "'''now timw to build the last sequential chain'''\n",
    "sequential_chain=prompt3 | model1 | parser \n",
    "\n",
    "'''final_chain'''\n",
    "final_chain= parallel_chain | sequential_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77c12dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Data Structures and Algorithms (DSA) Study Notes and Quiz**\n",
      "\n",
      "**I. Introduction**\n",
      "\n",
      "*   Definition: Data Structures and Algorithms are the building blocks of computer science.\n",
      "*   Importance: Understanding DSA is crucial for developing efficient and scalable software systems.\n",
      "*   Key Concepts:\n",
      "    *   Data Structures: Arrays, Linked Lists, Stacks, Queues, Trees, Graphs\n",
      "    *   Algorithms: Sorting, Searching, Graph Traversal, Dynamic Programming\n",
      "\n",
      "**II. Arrays**\n",
      "\n",
      "*   **Definition:** An array is a collection of elements of the same data type stored in contiguous memory locations.\n",
      "*   **Properties:**\n",
      "    *   Fixed size\n",
      "    *   Elements are accessed using an index\n",
      "    *   Supports random access\n",
      "*   **Operations:**\n",
      "    *   Insertion\n",
      "    *   Deletion\n",
      "    *   Searching\n",
      "    *   Sorting\n",
      "*   **Example Use Cases:**\n",
      "    *   Storing a list of numbers\n",
      "    *   Representing a matrix\n",
      "\n",
      "**III. Linked Lists**\n",
      "\n",
      "*   **Definition:** A linked list is a dynamic collection of elements, where each element points to the next element.\n",
      "*   **Properties:**\n",
      "    *   Dynamic size\n",
      "    *   Elements are accessed using a pointer\n",
      "    *   Supports insertion and deletion at any position\n",
      "*   **Types:**\n",
      "    *   Singly Linked List\n",
      "    *   Doubly Linked List\n",
      "    *   Circular Linked List\n",
      "*   **Operations:**\n",
      "    *   Insertion\n",
      "    *   Deletion\n",
      "    *   Searching\n",
      "    *   Sorting\n",
      "*   **Example Use Cases:**\n",
      "    *   Implementing a stack or queue\n",
      "    *   Representing a database index\n",
      "\n",
      "**IV. Stacks and Queues**\n",
      "\n",
      "*   **Definition:**\n",
      "    *   Stack: A Last-In-First-Out (LIFO) data structure\n",
      "    *   Queue: A First-In-First-Out (FIFO) data structure\n",
      "*   **Properties:**\n",
      "    *   Stack: Fixed size, supports push and pop operations\n",
      "    *   Queue: Dynamic size, supports enqueue and dequeue operations\n",
      "*   **Operations:**\n",
      "    *   Stack: Push, Pop, Peek\n",
      "    *   Queue: Enqueue, Dequeue, Peek\n",
      "*   **Example Use Cases:**\n",
      "    *   Evaluating postfix expressions\n",
      "    *   Implementing a browser's back button\n",
      "\n",
      "**V. Trees**\n",
      "\n",
      "*   **Definition:** A tree is a hierarchical data structure consisting of nodes with a parent-child relationship.\n",
      "*   **Properties:**\n",
      "    *   Root node\n",
      "    *   Child nodes\n",
      "    *   Leaf nodes\n",
      "*   **Types:**\n",
      "    *   Binary Tree\n",
      "    *   AVL Tree\n",
      "    *   Red-Black Tree\n",
      "*   **Operations:**\n",
      "    *   Insertion\n",
      "    *   Deletion\n",
      "    *   Searching\n",
      "    *   Traversal\n",
      "*   **Example Use Cases:**\n",
      "    *   File system organization\n",
      "    *   Database indexing\n",
      "\n",
      "**VI. Graphs**\n",
      "\n",
      "*   **Definition:** A graph is a non-linear data structure consisting of nodes connected by edges.\n",
      "*   **Properties:**\n",
      "    *   Nodes\n",
      "    *   Edges\n",
      "    *   Adjacency matrix or list\n",
      "*   **Types:**\n",
      "    *   Directed Graph\n",
      "    *   Undirected Graph\n",
      "    *   Weighted Graph\n",
      "*   **Operations:**\n",
      "    *   Insertion\n",
      "    *   Deletion\n",
      "    *   Searching\n",
      "    *   Traversal\n",
      "*   **Example Use Cases:**\n",
      "    *   Social network analysis\n",
      "    *   Route planning\n",
      "\n",
      "**VII. Sorting Algorithms**\n",
      "\n",
      "*   **Definition:** A sorting algorithm rearranges elements in a data structure to satisfy a specific order.\n",
      "*   **Types:**\n",
      "    *   Bubble Sort\n",
      "    *   Selection Sort\n",
      "    *   Insertion Sort\n",
      "    *   Merge Sort\n",
      "    *   Quick Sort\n",
      "*   **Properties:**\n",
      "    *   Time complexity\n",
      "    *   Space complexity\n",
      "*   **Example Use Cases:**\n",
      "    *   Sorting a list of numbers\n",
      "    *   Organizing a database\n",
      "\n",
      "**VIII. Searching Algorithms**\n",
      "\n",
      "*   **Definition:** A searching algorithm finds a specific element in a data structure.\n",
      "*   **Types:**\n",
      "    *   Linear Search\n",
      "    *   Binary Search\n",
      "*   **Properties:**\n",
      "    *   Time complexity\n",
      "    *   Space complexity\n",
      "*   **Example Use Cases:**\n",
      "    *   Finding a specific record in a database\n",
      "    *   Locating a file on a file system\n",
      "\n",
      "**IX. Dynamic Programming**\n",
      "\n",
      "*   **Definition:** Dynamic programming is a problem-solving approach that breaks down a complex problem into smaller sub-problems.\n",
      "*   **Properties:**\n",
      "    *   Overlapping sub-problems\n",
      "    *   Optimal sub-structure\n",
      "*   **Types:**\n",
      "    *   Memoization\n",
      "    *   Tabulation\n",
      "*   **Example Use Cases:**\n",
      "    *   Fibonacci sequence calculation\n",
      "    *   Shortest path problem\n",
      "\n",
      "**X. Conclusion**\n",
      "\n",
      "*   Data Structures and Algorithms are the foundation of computer science.\n",
      "*   Understanding DSA is crucial for developing efficient and scalable software systems.\n",
      "*   Practice and experience are key to mastering DSA concepts.\n",
      "\n",
      "**DSA Quiz**\n",
      "\n",
      "**Question 1:** What is the time complexity of the Bubble Sort algorithm?\n",
      "A) O(n log n)\n",
      "B) O(n^2)\n",
      "C) O(n)\n",
      "D) O(log n)\n",
      "\n",
      "**Answer:** B) O(n^2)\n",
      "\n",
      "**Question 2:** Which data structure is best suited for storing a collection of unique elements?\n",
      "A) Array\n",
      "B) Linked List\n",
      "C) Set\n",
      "D) Stack\n",
      "\n",
      "**Answer:** C) Set\n",
      "\n",
      "**Question 3:** What is the name of the algorithm used to find the closest pair of points in a set of points in 2D space?\n",
      "A) Dijkstra's algorithm\n",
      "B) Floyd-Warshall algorithm\n",
      "C) Closest Pair algorithm\n",
      "D) Breadth-First Search algorithm\n",
      "\n",
      "**Answer:** C) Closest Pair algorithm\n",
      "\n",
      "**Question 4:** What is the time complexity of the Merge Sort algorithm?\n",
      "A) O(n^2)\n",
      "B) O(n log n)\n",
      "C) O(n)\n",
      "D) O(log n)\n",
      "\n",
      "**Answer:** B) O(n log n)\n",
      "\n",
      "**Question 5:** Which data structure is used to implement a Last-In-First-Out (LIFO) stack?\n",
      "A) Queue\n",
      "B) Stack\n",
      "C) Linked List\n",
      "D) Tree\n",
      "\n",
      "**Answer:** B) Stack\n"
     ]
    }
   ],
   "source": [
    "# let's test\n",
    "\n",
    "topic=input(\"Please enter a topic to master: \")\n",
    "result= final_chain.invoke({'topic':topic})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4993d",
   "metadata": {},
   "source": [
    "# **04. Question Answering over Documents - RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b79c3",
   "metadata": {},
   "source": [
    "In this session  we'll learn about the question-answering over documents and embeddibgs and vectror database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943048f",
   "metadata": {},
   "source": [
    "## RAG (using vector embeddings) without connecting to an external database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09982806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19c272",
   "metadata": {},
   "source": [
    "Lets check if the langchain module is updated to the latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c15eef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4349f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch #without connecting to a vector DB you can use the lamgchain supported in-memory vector store\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eda0514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_groq = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"), \n",
    "    temperature=0.1,    # Controls randomness in responses\n",
    "    model_name=\"llama-3.1-8b-instant\"   # or any Groq-supported model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc5d6d",
   "metadata": {},
   "source": [
    "By inspection we saw that the csv file contains some special character which if directly passed to the default encoder will raise an error. This is why during loading the data we'll update the encoding configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1765a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'C:\\Users\\DEEPMALYA\\OneDrive\\Desktop\\pip_Malya\\Python\\GEN\\theory\\dataset01\\OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee99a0d",
   "metadata": {},
   "source": [
    "Next we'd have tp create the vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1361710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70034b",
   "metadata": {},
   "source": [
    "VectorstoreIndexCreator is a utility class or a pipeline builder in LangChain that helps you:\n",
    "\n",
    "Load documents (via loader)\n",
    "\n",
    "Embed those documents (using an embedding model you provide)\n",
    "\n",
    "Store them in a vectorstore (like FAISS(external vector DB), DocArrayInMemorySearch, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32089418",
   "metadata": {},
   "source": [
    "HiggingFaceEmbeddings is the wrapper that brings in all functionalities, but deep down it uses tools like Sentence Transformers for embeddings creation, thus we need to install the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f15f0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff5f97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#This is a langchain utility that creates an index from documents \n",
    "index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch,embedding=embedding_model).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0bfd0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c39d8d",
   "metadata": {},
   "source": [
    "Now its time to query the vector storage for an answer that has the context stored in forms of vectors and embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e919661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=groq_chat\n",
    "response=index.query(query,llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "539dad97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Name | Description | Fabric | UPF Rating |\n",
       "| --- | --- | --- | --- |\n",
       "| Sun Shield Shirt by | High-performance sun shirt with SPF 50+ protection, blocks 98% of UV rays | 78% nylon, 22% Lycra Xtra Life | 50+ |\n",
       "| Women's Tropical Tee, Sleeveless | Sleeveless button-up shirt with SunSmart protection, blocks 98% of UV rays | Shell: 71% nylon, 29% polyester, Cape lining: 100% polyester | 50+ |\n",
       "| Tropical Breeze Shirt | Lightweight, breathable long-sleeve shirt with SunSmart protection, blocks 98% of UV rays | Shell: 71% nylon, 29% polyester, Cape lining: 100% polyester | 50+ |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | Ultracomfortable sun protection shirt with UPF 50+ coverage, blocks 98% of UV rays | 52% polyester, 48% nylon | 50+ |\n",
       "\n",
       "Here's a brief summary of each shirt:\n",
       "\n",
       "* **Sun Shield Shirt by**: A high-performance sun shirt with SPF 50+ protection, perfect for outdoor activities.\n",
       "* **Women's Tropical Tee, Sleeveless**: A sleeveless button-up shirt with SunSmart protection, great for warm weather and water activities.\n",
       "* **Tropical Breeze Shirt**: A lightweight, breathable long-sleeve shirt with SunSmart protection, ideal for fishing, travel, or outdoor activities.\n",
       "* **Men's Plaid Tropic Shirt, Short-Sleeve**: An ultracomfortable sun protection shirt with UPF 50+ coverage, suitable for fishing, travel, or extended outdoor activities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efec554",
   "metadata": {},
   "source": [
    "![alt text](images.jpg) **YOOOOOOOOOOOOOOOOOOOOOOOOOO!!!!! First RAG implemented 🥳**|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e073238",
   "metadata": {},
   "source": [
    "## Details On Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d703ad8",
   "metadata": {},
   "source": [
    "**Deep Under The Hood - how it all works ?**\n",
    "\n",
    "Language model can at a time process around 1000 words from a document. But what if we have a dataset very big ? This is where we need vector store that stores the same piece of words as an embeddings.\n",
    "\n",
    "Embedding vectors captures content or meaning and texts with similar content have similar vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41c4a4",
   "metadata": {},
   "source": [
    "![alt text](1_SCNSHWA037wFjWMr1BmU5w.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329b699",
   "metadata": {},
   "source": [
    "**Vector Database**\n",
    "\n",
    "It is a warehouse to store the embeddings just created. When we get a big document we first break it down into smaller chunks. this create pieces of texts which are smaller thn the original document.We may not be able to pass the entire document to the LLM owing to it's size, thus we create small chunks.These chunks are then stored in the vector database as a form of embedding.\n",
    "\n",
    "now when a query comes in we again first create an embedding for the query to compare it with the existing embeddinbgs in the vector DB, then we pick the most similar one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48f43b",
   "metadata": {},
   "source": [
    "Expainig this workflow - step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "21db55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'C:\\Users\\DEEPMALYA\\OneDrive\\Desktop\\pip_Malya\\Python\\GEN\\theory\\dataset01\\OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "afc39719",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e2018e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\DEEPMALYA\\\\OneDrive\\\\Desktop\\\\pip_Malya\\\\Python\\\\GEN\\\\theory\\\\dataset01\\\\OutdoorClothingCatalog_1000.csv', 'row': 0}, page_content=\"\\ufeffUnnamed: 0: 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\r\\n\\r\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\r\\n\\r\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\r\\n\\r\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\r\\n\\r\\nQuestions? Please contact us for any inquiries.\")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9b4a5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c56e1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embedding_model.embed_query(\"Hi my name is Deepmalya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6dd9ee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84f08b",
   "metadata": {},
   "source": [
    "This shows that different vector chunks has been created in the n-dimensional vector space for the text I placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "308a7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05844542384147644, -0.05370223522186279, -0.02370099537074566, -0.019565032795071602, -0.11788613349199295]\n"
     ]
    }
   ],
   "source": [
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29914df",
   "metadata": {},
   "source": [
    "now  we want to store all the vector embeddings we created for the text into a vector db (here the implicit one supported by langchain). but this time let's noy automate this with the VectorstoreIndexCreator pipeline. let's manually do it ourselves for deeper understanding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "10e01104",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(docs, embedding_model)\n",
    "#this creates a vector store for the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7ca74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be8bad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e8fe524b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs) # there are 4 such similar answers to the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dff91809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\DEEPMALYA\\\\OneDrive\\\\Desktop\\\\pip_Malya\\\\Python\\\\GEN\\\\theory\\\\dataset01\\\\OutdoorClothingCatalog_1000.csv', 'row': 255}, page_content='\\ufeffUnnamed: 0: 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\r\\n\\r\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\r\\n\\r\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\r\\n\\r\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\r\\n\\r\\nSun Protection That Won\\'t Wear Off\\r\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1f0c0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''now for all the similar documents found from the vector storeb against ourn query we need to filter out the best again.\n",
    "This is why we first need to build a combined doc from the list of separated retrieved docs'''\n",
    "\n",
    "document_unit=\"\".join(docs[i].page_content for i in range(len(docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fc293c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(f\"{document_unit} Question: Please list all \\\n",
    "your shirts with sun protection in a table in markdown and summarize each one.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a18a855f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Sun Protection Shirts Table**\n",
       "\n",
       "| Name | Description | Fabric | UPF Rating | Features |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| Sun Shield Shirt by | Blocks 98% of sun's harmful rays, high-performance fabric | 78% nylon, 22% Lycra Xtra Life | 50+ | Moisture-wicking, abrasion-resistant, quick-drying |\n",
       "| Tropical Breeze Shirt | Lightweight, breathable long-sleeve shirt with superior SunSmart protection | 71% nylon, 29% polyester | 50+ | Wrinkle-resistant, moisture-wicking, front and back cape venting |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | Ultracomfortable sun protection, wrinkle-free and quickly evaporates perspiration | 52% polyester, 48% nylon | 50+ | Front and back cape venting, two front bellows pockets |\n",
       "| Women's Tropical Tee, Sleeveless | Five-star sleeveless button-up shirt with SunSmart protection | 71% nylon, 29% polyester | 50+ | Wrinkle-resistant, low-profile pockets, side shaping, front and back cape venting |\n",
       "\n",
       "**Summary of Each Shirt:**\n",
       "\n",
       "1. **Sun Shield Shirt by**: A high-performance sun shirt that blocks 98% of the sun's harmful rays, providing excellent protection and comfort.\n",
       "2. **Tropical Breeze Shirt**: A lightweight, breathable long-sleeve shirt designed for fishing and travel, offering superior SunSmart protection and innovative features like wrinkle-resistant fabric and front and back cape venting.\n",
       "3. **Men's Plaid Tropic Shirt, Short-Sleeve**: A comfortable and practical sun shirt designed for fishing and travel, featuring wrinkle-free fabric, front and back cape venting, and two front bellows pockets.\n",
       "4. **Women's Tropical Tee, Sleeveless**: A stylish and functional sleeveless button-up shirt with SunSmart protection, featuring a flattering fit, wrinkle-resistant fabric, and innovative features like low-profile pockets and side shaping."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Now the response we got is a structured response having Ai message object, not a plain string. Thus to get the exact content we want we need to :\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b3256908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this entire workflow can be guided and automated by the VectorstoreIndexCreator pipeline or by using Chains from langchain,\n",
    "# we need to build a retriever that helps us with the Q&A for our documents.\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "#We create a chain for the retrieval process passing it the parameters to work with:\n",
    "\n",
    "qa=RetrievalQA.from_chain_type(llm=model,chain_type='stuff',retriever=retriever,verbose=True) #chain_type=\"stuff\" says that stuff all the documents into a single context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2abdcc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1=\"Please list all your shirts with sun protection in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8f713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here's a table of shirts with sun protection:\n",
      "\n",
      "| Name | Description | Features |\n",
      "| --- | --- | --- |\n",
      "| Sun Shield Shirt by | High-performance sun shirt with UPF 50+ protection | Softly shapes the body, wicks moisture, abrasion resistant, handwash, line dry |\n",
      "| Women's Tropical Tee, Sleeveless | Five-star sleeveless button-up shirt with SunSmart protection | Slightly fitted, shell: 71% nylon, 29% polyester, UPF 50+ rated, machine wash and dry |\n",
      "| Sunrise Tee | Lightweight, high-performance UV-protective button down shirt | Slightly fitted, lightweight synthetic fabric, UPF 50+ rated, wrinkle-free, machine wash and dry |\n",
      "| Tropical Breeze Shirt | Lightweight, breathable long-sleeve UPF shirt with SunSmart protection | Traditional fit, wrinkle-resistant and moisture-wicking fabric, UPF 50+ rated, machine wash and dry |\n",
      "\n",
      "Here's a brief summary of each shirt:\n",
      "\n",
      "* The Sun Shield Shirt by is a high-performance sun shirt that provides excellent protection from the sun's harmful rays.\n",
      "* The Women's Tropical Tee, Sleeveless is a stylish and functional sleeveless shirt that offers great sun protection and a flattering fit.\n",
      "* The Sunrise Tee is a lightweight and breathable button down shirt that keeps you cool and protected from the sun.\n",
      "* The Tropical Breeze Shirt is a long-sleeve shirt that provides superior sun protection and a comfortable fit, making it perfect for outdoor activities.\n"
     ]
    }
   ],
   "source": [
    "response1=(query1) \n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b6386",
   "metadata": {},
   "source": [
    "# **05.Langchain - Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930ad32",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "Example generation\n",
    "\n",
    "Manual evaluation (and debuging)\n",
    "\n",
    "LLM-assisted evaluation\n",
    "\n",
    "LangChain evaluation platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb006130",
   "metadata": {},
   "source": [
    "We'll first create and then evaluate a Q&A application in this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54011691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e308c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'C:\\Users\\DEEPMALYA\\OneDrive\\Desktop\\pip_Malya\\Python\\GEN\\theory\\dataset01\\OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file,encoding='UTF-8')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd1f5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_25968\\2537902316.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54db60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b4df23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_chat = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"), \n",
    "    temperature=0.1,    # Controls randomness in responses\n",
    "    model_name=\"llama-3.1-8b-instant\"   # or any Groq-supported model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7135c1",
   "metadata": {},
   "source": [
    "**Storage logic (embeddings, indexing) lives in one module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adbad484",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch,embedding=embedding_model).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d989b2d",
   "metadata": {},
   "source": [
    "**Retrieval + reasoning logic lives in another**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae581e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = groq_chat\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", # stuff for small scal document q&a\n",
    "    retriever=index.vectorstore.as_retriever(), \n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035389eb",
   "metadata": {},
   "source": [
    "Evaluate using favourable data - points from the data given to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55c98019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\DEEPMALYA\\\\OneDrive\\\\Desktop\\\\pip_Malya\\\\Python\\\\GEN\\\\theory\\\\dataset01\\\\OutdoorClothingCatalog_1000.csv', 'row': 10}, page_content=\"\\ufeffUnnamed: 0: 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\r\\n\\r\\nSize & Fit\\r\\n- Pants are Favorite Fit: Sits lower on the waist.\\r\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\r\\n\\r\\nFabric & Care\\r\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\r\\n\\r\\nAdditional Features\\r\\n- Relaxed fit top with raglan sleeves and rounded hem.\\r\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\r\\n\\r\\nImported.\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61325ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\DEEPMALYA\\\\OneDrive\\\\Desktop\\\\pip_Malya\\\\Python\\\\GEN\\\\theory\\\\dataset01\\\\OutdoorClothingCatalog_1000.csv', 'row': 11}, page_content='\\ufeffUnnamed: 0: 11\\nname: Ultra-Lofty 850 Stretch Down Hooded Jacket\\ndescription: This technical stretch down jacket from our DownTek collection is sure to keep you warm and comfortable with its full-stretch construction providing exceptional range of motion. With a slightly fitted style that falls at the hip and best with a midweight layer, this jacket is suitable for light activity up to 20° and moderate activity up to -30°. The soft and durable 100% polyester shell offers complete windproof protection and is insulated with warm, lofty goose down. Other features include welded baffles for a no-stitch construction and excellent stretch, an adjustable hood, an interior media port and mesh stash pocket and a hem drawcord. Machine wash and dry. Imported.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53612912",
   "metadata": {},
   "source": [
    "**Hard Coded Examples - good queries as a test point from the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45297fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
    "        have side pockets?\",\n",
    "        \"answer\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What collection is the Ultra-Lofty \\\n",
    "        850 Stretch Down Hooded Jacket from?\",\n",
    "        \"answer\": \"The DownTek collection\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97b7f713",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m(examples)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "qa.run(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda799e",
   "metadata": {},
   "source": [
    "**LLM generated examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9eb79c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d264b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen_chain = QAGenerateChain.from_llm(groq_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b30775a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_25968\\3125874183.py:1: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  new_examples = example_gen_chain.apply_and_parse(\n"
     ]
    }
   ],
   "source": [
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in data[:5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eb08f619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_pairs': {'query': \"What type of material is used for the innersole of the Women's Campside Oxfords, and what feature does it have to prevent odor buildup?\",\n",
       "  'answer': \"The innersole of the Women's Campside Oxfords is made of EVA (Ethylene-Vinyl Acetate) material, and it features Cleansport NXT antimicrobial odor control.\"}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2ae98b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\DEEPMALYA\\\\OneDrive\\\\Desktop\\\\pip_Malya\\\\Python\\\\GEN\\\\theory\\\\dataset01\\\\OutdoorClothingCatalog_1000.csv', 'row': 0}, page_content=\"\\ufeffUnnamed: 0: 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\r\\n\\r\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\r\\n\\r\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\r\\n\\r\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\r\\n\\r\\nQuestions? Please contact us for any inquiries.\")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b30a4",
   "metadata": {},
   "source": [
    "### Combine examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8ffe3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples+=new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9e2c4fe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m(examples[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa54b60",
   "metadata": {},
   "source": [
    "# **#Langchain Special - Runnables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce64040",
   "metadata": {},
   "source": [
    "This defines how chains works internally ....**A deeper insight to Langchain framework**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e754f61",
   "metadata": {},
   "source": [
    "**Runnables** - the engine that runs the chains and ultimately the entire langchain framework\n",
    "\n",
    "While building a project with AI - say building a PDF content extractor, other than interacting with LLM APIs, there are several other aspects in the workkflow - the Document loaders, splitting data into chunks, converting them into embeddings , storing them in a db, retrieving the content, using LLM and NLP for the output, parsing it as a structured output to the user. This is where langchian decided to create different classes for these respective Operations so that building a project using this framework becomes easier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac7b02",
   "metadata": {},
   "source": [
    "## Let's build a PDF Reader system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2c9366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8be4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import csv_loader,TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document\n",
    "\n",
    "loader=TextLoader(\"docx.text\") # routes to the filepath\n",
    "documents=loader.load() # reads the contents from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c3d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into smaller chunks\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50) #chunk size defines the number of words to create an embedding for in a single go...while overlap defines the text continuity to preserve context\n",
    "docs=text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "511d048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPMALYA\\AppData\\Local\\Temp\\ipykernel_26096\\3681082152.py:1: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  vector_store=Chroma.from_documents(docs,HuggingFaceBgeEmbeddings())\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected Embeddings to be non-empty list or numpy array, got [] in upsert.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vector_store=\u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mHuggingFaceBgeEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:326\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m         \u001b[38;5;28mself\u001b[39m._collection.upsert(\n\u001b[32m    321\u001b[39m             embeddings=embeddings_without_metadatas,\n\u001b[32m    322\u001b[39m             documents=texts_without_metadatas,\n\u001b[32m    323\u001b[39m             ids=ids_without_metadatas,\n\u001b[32m    324\u001b[39m         )\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:365\u001b[39m, in \u001b[36mCollection.upsert\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupsert\u001b[39m(\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    342\u001b[39m     ids: OneOrMany[ID],\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     uris: Optional[OneOrMany[URI]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    353\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    354\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m \u001b[33;03m        None\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     upsert_request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_prepare_upsert_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m._client._upsert(\n\u001b[32m    375\u001b[39m         collection_id=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    376\u001b[39m         ids=upsert_request[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m         database=\u001b[38;5;28mself\u001b[39m.database,\n\u001b[32m    383\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:95\u001b[39m, in \u001b[36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> T:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     97\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:405\u001b[39m, in \u001b[36mCollectionCommon._validate_and_prepare_upsert_request\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;129m@validation_context\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mupsert\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_and_prepare_upsert_request\u001b[39m(\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m ) -> UpsertRequest:\n\u001b[32m    404\u001b[39m     \u001b[38;5;66;03m# Unpack\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     upsert_records = \u001b[43mnormalize_insert_record_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    415\u001b[39m     validate_insert_record_set(record_set=upsert_records)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\chromadb\\api\\types.py:271\u001b[39m, in \u001b[36mnormalize_insert_record_set\u001b[39m\u001b[34m(ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_insert_record_set\u001b[39m(\n\u001b[32m    256\u001b[39m     ids: OneOrMany[ID],\n\u001b[32m    257\u001b[39m     embeddings: Optional[\n\u001b[32m   (...)\u001b[39m\u001b[32m    266\u001b[39m     uris: Optional[OneOrMany[URI]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    267\u001b[39m ) -> InsertRecordSet:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Unpacks and normalizes the fields of an InsertRecordSet.\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     base_record_set = \u001b[43mnormalize_base_record_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43muris\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m InsertRecordSet(\n\u001b[32m    276\u001b[39m         ids=cast(IDs, maybe_cast_one_to_many(ids)),\n\u001b[32m    277\u001b[39m         metadatas=maybe_cast_one_to_many(metadatas),\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m         uris=base_record_set[\u001b[33m\"\u001b[39m\u001b[33muris\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    282\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\chromadb\\api\\types.py:248\u001b[39m, in \u001b[36mnormalize_base_record_set\u001b[39m\u001b[34m(embeddings, documents, images, uris)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_base_record_set\u001b[39m(\n\u001b[32m    238\u001b[39m     embeddings: Optional[Union[OneOrMany[Embedding], OneOrMany[PyEmbedding]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    239\u001b[39m     documents: Optional[OneOrMany[Document]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    240\u001b[39m     images: Optional[OneOrMany[Image]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    241\u001b[39m     uris: Optional[OneOrMany[URI]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    242\u001b[39m ) -> BaseRecordSet:\n\u001b[32m    243\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[33;03m    Unpacks and normalizes the fields of a BaseRecordSet.\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BaseRecordSet(\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m         embeddings=\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    249\u001b[39m         documents=maybe_cast_one_to_many(documents),\n\u001b[32m    250\u001b[39m         images=maybe_cast_one_to_many(images),\n\u001b[32m    251\u001b[39m         uris=maybe_cast_one_to_many(uris),\n\u001b[32m    252\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\chromadb\\api\\types.py:145\u001b[39m, in \u001b[36mnormalize_embeddings\u001b[39m\u001b[34m(target)\u001b[39m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    146\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected Embeddings to be non-empty list or numpy array, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# One PyEmbedding\u001b[39;00m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target[\u001b[32m0\u001b[39m], (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target[\u001b[32m0\u001b[39m], \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: Expected Embeddings to be non-empty list or numpy array, got [] in upsert."
     ]
    }
   ],
   "source": [
    "vector_store=Chroma.from_documents(docs,HuggingFaceBgeEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3fc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
